{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import io_tools as iot\n",
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "import plyvel\n",
    "from caffe import io\n",
    "import sys\n",
    "\n",
    "import cPickle as pickle\n",
    "import gzip\n",
    "\n",
    "from transformers import *\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion, make_union\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "# DATA_DIR = iot.DATA_DIR\n",
    "DATA_DIR = '/media/raid_arr/data/otto/data/'\n",
    "DB_OUT_TRAIN = os.path.join(DATA_DIR, 'train_lvl')\n",
    "DB_OUT_TEST = os.path.join(DATA_DIR, 'test_lvl')\n",
    "DB_OUT_ALL = os.path.join(DATA_DIR, 'all_lvl')\n",
    "DB_OUT_ALL0 = os.path.join(DATA_DIR, 'all0_lvl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ids_train, feats_train, labels_train\n",
    "except NameError:\n",
    "    ids_train = np.load(os.path.join(iot.DATA_DIR, 'train_ids.npy'))\n",
    "    feats_train = np.load(os.path.join(iot.DATA_DIR, 'train_feats.npy')).astype(float)\n",
    "    labels_train = np.load(os.path.join(iot.DATA_DIR, 'train_labels_enc.npy'))\n",
    "\n",
    "try:\n",
    "    ids_test, feats_test\n",
    "except NameError:\n",
    "    ids_test = np.load(os.path.join(iot.DATA_DIR, 'test_ids.npy'))\n",
    "    feats_test = np.load(os.path.join(iot.DATA_DIR, 'test_feats.npy')).astype(float)   \n",
    "    \n",
    "skf = StratifiedKFold(labels_train, n_folds=5, shuffle=True)\n",
    "# All\n",
    "feats_all = np.r_[feats_train, feats_test]\n",
    "labels_all = np.r_[labels_train, -1*np.ones(len(ids_test))].astype(int)\n",
    "\n",
    "# All minus test0\n",
    "train_ind, val_ind = iter(skf).next()\n",
    "feats_all0 = np.r_[feats_train[train_ind, :], feats_test]\n",
    "labels_all0 = np.r_[labels_train[train_ind], -1*np.ones(len(ids_test))].astype(int)\n",
    "\n",
    "\n",
    "feats_fold_train = feats_train[train_ind, :]\n",
    "labels_fold_train = labels_train[train_ind]\n",
    "feats_fold_val = feats_train[val_ind, :]\n",
    "labels_fold_val = labels_train[val_ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('logtformer', <__main__.LogTformer object at 0x7f980c89d650>), ('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(LogTformer(), StandardScaler())\n",
    "pipe.fit(np.r_[feats_train[train_ind, :], feats_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('identitytformer', <__main__.IdentityTformer object at 0x7f97c2bdb250>), ('logtformer', <__main__.LogTformer object at 0x7f97c2bdbcd0>), ('anscombetformer', <__main__.AnscombeTformer object at 0x7f97c2bdb2d0>), ('inversetformer', <__main__.InverseTformer object at 0x7f97c2bdb690>)],\n",
       "       transformer_weights=None)), ('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(\n",
    "    make_union(\n",
    "        IdentityTformer(), LogTformer(), AnscombeTformer(), InverseTformer(),\n",
    "    ), StandardScaler()\n",
    "                    )\n",
    "pipe.fit(np.r_[feats_train[train_ind, :], feats_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/numpy/core/_methods.py:83: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  warnings.warn(\"Degrees of freedom <= 0 for slice\", RuntimeWarning)\n",
      "/usr/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidftransformer', TfidfTransformer(norm=u'l2', smooth_idf=True, sublinear_tf=True, use_idf=True)), ('densetformer', <__main__.DenseTformer object at 0x7f1da68c4090>), ('featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('identitytformer', <__main__.IdentityTformer object at 0x...former_weights=None)), ('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True))])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(\n",
    "                    TfidfTransformer(norm=u'l2', \n",
    "                                      use_idf=True, \n",
    "                                      smooth_idf=True, \n",
    "                                      sublinear_tf=True),\n",
    "                     DenseTformer(),\n",
    "                     make_union(\n",
    "                                IdentityTformer(),\n",
    "#                                 FactorAnalysis(n_components=74),\n",
    "#                                 PCA(n_components=20, whiten=True),\n",
    "                                NzTformer(),\n",
    "                                NzvarTformer(),\n",
    "                                NzmeanTformer(),\n",
    "                     ),\n",
    "                     StandardScaler(),\n",
    "#                      MinMaxScaler(),\n",
    "                     )\n",
    "pipe.fit(np.r_[feats_train[train_ind, :], feats_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = pipe.transform(feats_fold_train)\n",
    "y_train = labels_fold_train\n",
    "\n",
    "X_valid = pipe.transform(feats_fold_val)\n",
    "y_valid = labels_fold_val\n",
    "\n",
    "X_test = pipe.transform(feats_test)\n",
    "y_test = -1*np.ones(len(feats_test), dtype=int)\n",
    "\n",
    "data = ((X_train, y_train),\n",
    "        (X_valid, y_valid),\n",
    "        (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144368, 372)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(data, gzip.open('/media/raid_arr/data/otto/data/iden-log-anscombe-inv_data.pgz', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-44ff2fb50a85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats_fold_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels_fold_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats_fold_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels_fold_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/ee.cooper.edu/user/t/a/tam8/documents/scikit-learn/sklearn/utils/metaestimators.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/ee.cooper.edu/user/t/a/tam8/documents/scikit-learn/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/ee.cooper.edu/user/t/a/tam8/documents/scikit-learn/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, y, copy)\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mscale\u001b[0m \u001b[0malong\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'std_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/ee.cooper.edu/user/t/a/tam8/documents/scikit-learn/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 622\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "def my_load():\n",
    "    \n",
    "#     X_train = pipe.transform(feats_fold_train)\n",
    "    y_train = labels_fold_train\n",
    "\n",
    "#     X_valid = pipe.transform(feats_fold_val)\n",
    "    y_valid = labels_fold_val\n",
    "\n",
    "#     X_test = pipe.transform(feats_test)\n",
    "    y_test = -1*np.ones(len(feats_test), dtype=int)\n",
    "\n",
    "    data = ((X_train, y_train),\n",
    "            (X_valid, y_valid),\n",
    "            (X_test, y_test))\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LESGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import gzip\n",
    "import itertools\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "\n",
    "PY2 = sys.version_info[0] == 2\n",
    "\n",
    "if PY2:\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "    def pickle_load(f, encoding):\n",
    "        return pickle.load(f)\n",
    "else:\n",
    "    from urllib.request import urlretrieve\n",
    "\n",
    "    def pickle_load(f, encoding):\n",
    "        return pickle.load(f, encoding=encoding)\n",
    "\n",
    "DATA_URL = 'http://deeplearning.net/data/mnist/mnist.pkl.gz'\n",
    "DATA_FILENAME = 'mnist.pkl.gz'\n",
    "\n",
    "NUM_EPOCHS = 5000\n",
    "BATCH_SIZE = 2048\n",
    "NUM_HIDDEN_UNITS = 1024\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _load_data(url=DATA_URL, filename=DATA_FILENAME):\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Downloading MNIST\")\n",
    "        urlretrieve(url, filename)\n",
    "\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        return pickle_load(f, encoding='latin-1')\n",
    "\n",
    "\n",
    "def load_data():\n",
    "#     data = _load_data()\n",
    "    data = my_load()\n",
    "    X_train, y_train = data[0]\n",
    "    X_valid, y_valid = data[1]\n",
    "    X_test, y_test = data[2]\n",
    "\n",
    "    return dict(\n",
    "        X_train=theano.shared(lasagne.utils.floatX(X_train)),\n",
    "        y_train=T.cast(theano.shared(y_train), 'int32'),\n",
    "        X_valid=theano.shared(lasagne.utils.floatX(X_valid)),\n",
    "        y_valid=T.cast(theano.shared(y_valid), 'int32'),\n",
    "        X_test=theano.shared(lasagne.utils.floatX(X_test)),\n",
    "        y_test=T.cast(theano.shared(y_test), 'int32'),\n",
    "        num_examples_train=X_train.shape[0],\n",
    "        num_examples_valid=X_valid.shape[0],\n",
    "        num_examples_test=X_test.shape[0],\n",
    "        input_dim=X_train.shape[1],\n",
    "        output_dim=len(np.unique(y_train)),\n",
    "    )\n",
    "\n",
    "\n",
    "def build_model(input_dim, output_dim,\n",
    "                batch_size=BATCH_SIZE, num_hidden_units=NUM_HIDDEN_UNITS):\n",
    "\n",
    "    l_in = lasagne.layers.InputLayer(\n",
    "        shape=(batch_size, input_dim),\n",
    "    )\n",
    "    l_hidden1 = lasagne.layers.DenseLayer(\n",
    "        l_in,\n",
    "        num_units=num_hidden_units,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "#         nonlinearity=lasagne.nonlinearities.identity,\n",
    "    )\n",
    "    \n",
    "#     lh1m = lasagne.layers.FeaturePoolLayer(\n",
    "#         l_hidden1,\n",
    "#         ds=2,\n",
    "#     )\n",
    "    \n",
    "    l_hidden1_dropout = lasagne.layers.DropoutLayer(\n",
    "        l_hidden1,\n",
    "        p=0.5,\n",
    "    )\n",
    "\n",
    "    l_hidden2 = lasagne.layers.DenseLayer(\n",
    "        l_hidden1_dropout,\n",
    "        num_units=num_hidden_units,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "#         nonlinearity=lasagne.nonlinearities.identity,\n",
    "    )\n",
    "    \n",
    "#     lh2m = lasagne.layers.FeaturePoolLayer(\n",
    "#         l_hidden2,\n",
    "#         ds=2,\n",
    "#     )\n",
    "    \n",
    "    \n",
    "    l_hidden2_dropout = lasagne.layers.DropoutLayer(\n",
    "        l_hidden2,\n",
    "        p=0.5,\n",
    "    )\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "        l_hidden2_dropout,\n",
    "        num_units=output_dim,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax,\n",
    "    )\n",
    "    return l_out\n",
    "\n",
    "\n",
    "def create_iter_functions(dataset, output_layer,\n",
    "                          X_tensor_type=T.matrix,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          learning_rate=LEARNING_RATE, momentum=MOMENTUM):\n",
    "    batch_index = T.iscalar('batch_index')\n",
    "    X_batch = X_tensor_type('x')\n",
    "    y_batch = T.ivector('y')\n",
    "    batch_slice = slice(\n",
    "        batch_index * batch_size, (batch_index + 1) * batch_size)\n",
    "\n",
    "    objective = lasagne.objectives.Objective(output_layer,\n",
    "        loss_function=lasagne.objectives.categorical_crossentropy)\n",
    "\n",
    "    loss_train = objective.get_loss(X_batch, target=y_batch)\n",
    "    loss_eval = objective.get_loss(X_batch, target=y_batch,\n",
    "                                   deterministic=True)\n",
    "\n",
    "    pred = T.argmax(\n",
    "        output_layer.get_output(X_batch, deterministic=True), axis=1)\n",
    "    accuracy = T.mean(T.eq(pred, y_batch), dtype=theano.config.floatX)\n",
    "\n",
    "    all_params = lasagne.layers.get_all_params(output_layer)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "        loss_train, all_params, learning_rate, momentum)\n",
    "\n",
    "    iter_train = theano.function(\n",
    "        [batch_index], loss_train,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            X_batch: dataset['X_train'][batch_slice],\n",
    "            y_batch: dataset['y_train'][batch_slice],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    iter_valid = theano.function(\n",
    "        [batch_index], [loss_eval, accuracy],\n",
    "        givens={\n",
    "            X_batch: dataset['X_valid'][batch_slice],\n",
    "            y_batch: dataset['y_valid'][batch_slice],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    iter_test = theano.function(\n",
    "        [batch_index], [loss_eval, accuracy],\n",
    "        givens={\n",
    "            X_batch: dataset['X_test'][batch_slice],\n",
    "            y_batch: dataset['y_test'][batch_slice],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "        train=iter_train,\n",
    "        valid=iter_valid,\n",
    "        test=iter_test,\n",
    "    )\n",
    "\n",
    "\n",
    "def train(iter_funcs, dataset, batch_size=BATCH_SIZE):\n",
    "    num_batches_train = dataset['num_examples_train'] // batch_size\n",
    "    num_batches_valid = dataset['num_examples_valid'] // batch_size\n",
    "\n",
    "    for epoch in itertools.count(1):\n",
    "        batch_train_losses = []\n",
    "        for b in range(num_batches_train):\n",
    "            batch_train_loss = iter_funcs['train'](b)\n",
    "            batch_train_losses.append(batch_train_loss)\n",
    "\n",
    "        avg_train_loss = np.mean(batch_train_losses)\n",
    "\n",
    "        batch_valid_losses = []\n",
    "        batch_valid_accuracies = []\n",
    "        for b in range(num_batches_valid):\n",
    "            batch_valid_loss, batch_valid_accuracy = iter_funcs['valid'](b)\n",
    "            batch_valid_losses.append(batch_valid_loss)\n",
    "            batch_valid_accuracies.append(batch_valid_accuracy)\n",
    "\n",
    "        avg_valid_loss = np.mean(batch_valid_losses)\n",
    "        avg_valid_accuracy = np.mean(batch_valid_accuracies)\n",
    "\n",
    "        yield {\n",
    "            'number': epoch,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'valid_loss': avg_valid_loss,\n",
    "            'valid_accuracy': avg_valid_accuracy,\n",
    "        }\n",
    "\n",
    "\n",
    "def main(num_epochs=NUM_EPOCHS):\n",
    "    print(\"Loading data...\")\n",
    "    dataset = load_data()\n",
    "\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    output_layer = build_model(\n",
    "        input_dim=dataset['input_dim'],\n",
    "        output_dim=dataset['output_dim'],\n",
    "    )\n",
    "    iter_funcs = create_iter_functions(dataset, output_layer)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    now = time.time()\n",
    "    try:\n",
    "        for epoch in train(iter_funcs, dataset):\n",
    "            if (epoch['number']-1) % 100 == 0:\n",
    "#             if 1:\n",
    "                print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "                    epoch['number'], num_epochs, time.time() - now))\n",
    "                now = time.time()\n",
    "                print(\"  training loss:\\t\\t{:.6f}\".format(epoch['train_loss']))\n",
    "                print(\"  validation loss:\\t\\t{:.6f}\".format(epoch['valid_loss']))\n",
    "                print(\"  validation accuracy:\\t\\t{:.2f} %%\".format(\n",
    "                    epoch['valid_accuracy'] * 100))\n",
    "                sys.stdout.flush()\n",
    "            if epoch['number'] >= num_epochs:\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 5000 took 0.444s\n",
      "  training loss:\t\t2.877856\n",
      "  validation loss:\t\t1.982359\n",
      "  validation accuracy:\t\t28.91 %%\n",
      "Epoch 101 of 5000 took 44.289s\n",
      "  training loss:\t\t0.613216\n",
      "  validation loss:\t\t0.557774\n",
      "  validation accuracy:\t\t79.12 %%\n",
      "Epoch 201 of 5000 took 44.288s\n",
      "  training loss:\t\t0.534340\n",
      "  validation loss:\t\t0.517341\n",
      "  validation accuracy:\t\t80.06 %%\n",
      "Epoch 301 of 5000 took 44.296s\n",
      "  training loss:\t\t0.478634\n",
      "  validation loss:\t\t0.497502\n",
      "  validation accuracy:\t\t80.54 %%\n",
      "Epoch 401 of 5000 took 44.307s\n",
      "  training loss:\t\t0.440227\n",
      "  validation loss:\t\t0.487613\n",
      "  validation accuracy:\t\t80.79 %%\n",
      "Epoch 501 of 5000 took 44.310s\n",
      "  training loss:\t\t0.406042\n",
      "  validation loss:\t\t0.482963\n",
      "  validation accuracy:\t\t81.09 %%\n",
      "Epoch 601 of 5000 took 44.306s\n",
      "  training loss:\t\t0.376894\n",
      "  validation loss:\t\t0.483602\n",
      "  validation accuracy:\t\t81.14 %%\n",
      "Epoch 701 of 5000 took 44.305s\n",
      "  training loss:\t\t0.350270\n",
      "  validation loss:\t\t0.486979\n",
      "  validation accuracy:\t\t81.20 %%\n",
      "Epoch 801 of 5000 took 44.308s\n",
      "  training loss:\t\t0.327211\n",
      "  validation loss:\t\t0.492335\n",
      "  validation accuracy:\t\t81.04 %%\n",
      "Epoch 901 of 5000 took 44.302s\n",
      "  training loss:\t\t0.306492\n",
      "  validation loss:\t\t0.498371\n",
      "  validation accuracy:\t\t81.14 %%\n",
      "Epoch 1001 of 5000 took 44.307s\n",
      "  training loss:\t\t0.287131\n",
      "  validation loss:\t\t0.506043\n",
      "  validation accuracy:\t\t81.10 %%\n",
      "Epoch 1101 of 5000 took 44.311s\n",
      "  training loss:\t\t0.273643\n",
      "  validation loss:\t\t0.513454\n",
      "  validation accuracy:\t\t81.26 %%\n",
      "Epoch 1201 of 5000 took 44.309s\n",
      "  training loss:\t\t0.256485\n",
      "  validation loss:\t\t0.521700\n",
      "  validation accuracy:\t\t81.40 %%\n",
      "Epoch 1301 of 5000 took 44.300s\n",
      "  training loss:\t\t0.238083\n",
      "  validation loss:\t\t0.530945\n",
      "  validation accuracy:\t\t81.40 %%\n",
      "Epoch 1401 of 5000 took 44.302s\n",
      "  training loss:\t\t0.226264\n",
      "  validation loss:\t\t0.538526\n",
      "  validation accuracy:\t\t81.27 %%\n",
      "Epoch 1501 of 5000 took 44.300s\n",
      "  training loss:\t\t0.219408\n",
      "  validation loss:\t\t0.548363\n",
      "  validation accuracy:\t\t81.40 %%\n"
     ]
    }
   ],
   "source": [
    "out_layer = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data = _load_data()\n",
    "data = my_load()\n",
    "X_train, y_train = data[0]\n",
    "X_valid, y_valid = data[1]\n",
    "X_test, y_test = data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61878, 93)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
